### Multi-Armed Recommender System Bandit Ensembles

El paper proponia un sistema recomendador que usara multi-armed bandits, el cual consiste en la siguiente idea: se tienen multiples botones de los que se obtiene una recompensa al presionarlos, el sistema buscara basicamente maximizar esta ganancia, por lo que presionara aquellos con mayor recompensa. Siguiendo esta idea, los botones eran distintos recomendadores, obteniendose cierta "recompensa" al seleccionarlos para hacer la recomendacion, asi el sistema aprendia que recomendador usar. Ademas probaron distintos bandits, el Thompson sampling bandit y el E-greedy bandit.

me habria gustado ver como el multi-armed bandit cambia los brazos que usa, a lo largo del tiempo, ya que los rewards no son estaticos. Por ejemplo un analisis de que tan suceptivo es a los cambios.

A pesar de lo anterior me gusta la idea porque permite poner algoritmos que funcionen bien incluso con el cool start y algoritmos que despues de este funcionen mejor. Como podemos apreciar en el paper donde el multi-armed bandit selecciona al comienzo most popular y despues usa matrix factozation. 

Para concluir, me parecio interesante agregar esta idea de Reinforcement Learning al sistema recomendador, y tiene la ventaja de tener bajo costo computacional en comparacion a los ensambles por lo que es una idea atractiva.













































































